{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PQ Prediction with HuBERT using a Deep Neural Net \n",
    "\n",
    "In the last notebook, we used HuBERT layers to predict the PQ Representation of the audio clips in the Perceptual Voice Qualities Database (PVQD). We used a naive aggregation technique to do this (simply averaging over the time steps), but what if instead we decided to try and take advantage of the temporal information to perform prediction? \n",
    "\n",
    "We may *want* to use deep neural networks, because they are super duper cool and powerful, but we have to ask if it is the best tool for the job. Given that the PQ Representation is a vector that represents speaker identity, which can be thought of as the time-invariant characteristics of speech (mostly), will the temporal information increase predictive performance? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.utils import *\n",
    "\n",
    "import time\n",
    "from collections import OrderedDict # Not necessary\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Rate: 16000\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Load in the HuBERT Model\n",
    "torch.random.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "bundle = torchaudio.pipelines.HUBERT_BASE\n",
    "hubert_model = bundle.get_model().to(device)\n",
    "hubert_model.eval() # make sure it's in eval mode\n",
    "\n",
    "print(\"Sample Rate:\", bundle.sample_rate)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the Data \n",
    "## Load in the DataFrame\n",
    "y_train = pd.read_csv('../data/pvqd/train_test_split/y_train.csv', index_col=0)\n",
    "y_val = pd.read_csv('../data/pvqd/train_test_split/y_val.csv', index_col=0)\n",
    "\n",
    "# Same inefficient code as before!\n",
    "\n",
    "data_path = \"../data/pvqd/audio_clips/\"\n",
    "audio_files = os.listdir(data_path)\n",
    "speaker_ids = [extract_speaker(audio_file) for audio_file in audio_files]\n",
    "\n",
    "# Assertion to make sure speaker_ids matches y_train['File']\n",
    "i = 0\n",
    "for spk_id in y_train[\"File\"]:\n",
    "    try:\n",
    "        assert spk_id in speaker_ids\n",
    "    except:\n",
    "        print(spk_id)\n",
    "        i+=1\n",
    "\n",
    "# Dictionary to Link Speaker ID to Audio File for O(1) access\n",
    "speaker_file_dict = {}\n",
    "for i in range(0, len(speaker_ids)):\n",
    "    speaker_file_dict[speaker_ids[i]] = os.path.join(data_path, audio_files[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First things first, we gotta make the data compatible with PyTorch \n",
    "\n",
    "PyTorch is a very powerful package that allows us to process data in fast and remarkable ways. Now, the way to take advantage of PyTorch's [Dataset](https://pytorch.org/tutorials/ beginner/data_loading_tutorial.html) class. Using this class, you can train in parallel, and easily modify batch size (the number of input samples your network considers per training update)\n",
    "\n",
    "Below, we're going to load in the hubert model and implement the class HubertDataset, which will take in a a dataframe of labels, the speaker-to-wavfile array, and a hubert model, and will return the sixth layer of the hubert features and the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Implement the Dataset function __getitem__ \n",
    "\n",
    "class HubertDataset(Dataset):\n",
    "    def __init__(self, dataframe, spk_wav_arr, hubert_model, device):\n",
    "        self.dataframe = dataframe\n",
    "        self.spk_wav_arr = spk_wav_arr # array that links speaker id and \n",
    "        self.pad_len = 1135467 # Max seq length\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of data samples\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    # TODO: Implement the __getitem__ function\n",
    "    # Given an index, return the features and the labels\n",
    "    def __getitem__(self, index):\n",
    "        # Retrieve a single row from the DataFrame\n",
    "        single_row = self.dataframe.values[index, :]\n",
    "        speaker_id = single_row[0]\n",
    "        audio_file = self.spk_wav_arr[speaker_id]\n",
    "\n",
    "        waveform, sample_rate = torchaudio.load(audio_file)\n",
    "\n",
    "        if sample_rate != bundle.sample_rate:\n",
    "            waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)\n",
    "\n",
    "        # Let's take a fixed length-- pad the waveform with 0s up to the max file length\n",
    "        # Question: Should we be padding with 0s?\n",
    "        to_pad = self.pad_len - waveform.shape[-1]\n",
    "        waveform = F.pad(waveform, (0, to_pad))\n",
    "\n",
    "        # TODO: Downmix Stereo to Mono & Get rid of the channels dimension\n",
    "        if len(waveform.shape) == 2 and (waveform.shape[0] == 2):\n",
    "            waveform = torch.mean(waveform, dim=0)\n",
    "\n",
    "        else:\n",
    "            waveform = waveform.squeeze(0)\n",
    "\n",
    "        # Load in the Labels\n",
    "        labels = torch.from_numpy(single_row[1:].astype(float))\n",
    "\n",
    "        \n",
    "\n",
    "        return waveform, labels\n",
    "\n",
    "\n",
    "def process_wavform(waveform, hubert_model):\n",
    "    # Extract the HuBERT layers like before\n",
    "    features = hubert_model.extract_features(waveform)\n",
    "    features = features[0]\n",
    "\n",
    "    features = features[5]\n",
    "\n",
    "    # Have to swap the axes, since we want the hubert dimension to be the channels\n",
    "    features = torch.swapaxes(features, 1, 2)\n",
    "\n",
    "    # Remove the first dimension\n",
    "    features = features.squeeze(0)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: PyTorch's Dataset class is *incredibly* flexible\n",
    "\n",
    "There's no need to make it load a pandas DataFrame at all. You could store everything on disk, and have the index be associated with a file location.\n",
    "\n",
    "Let's use our created HubertDataset class to create a DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HubertDataset(y_train, speaker_file_dict, hubert_model, device)\n",
    "\n",
    "batch_size = 8 # Set the batch size\n",
    "data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Moment You've All Been Waiting for: Let's Build a Neural Network\n",
    "\n",
    "For the purposes of this workshop, we're going to just build a convolutional neural network. There are other ways to model speech or sequential data more broadly, such as using a Transformer (or a Conformer if you want to combine the two). We're going to keep it simple. Feel free to modify the network though, Torchaudio makes it pretty easy! (https://pytorch.org/audio/main/generated/torchaudio.models.Conformer.html for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This class is not implemented, how do I put this, well. How would you change this class to make it more flexible and better?\n",
    "class ClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "\n",
    "      self.conv_block1 = nn.Sequential(OrderedDict([\n",
    "        ('conv', nn.Conv1d(768,256,5, padding=2)),\n",
    "        ('norm', nn.InstanceNorm1d(256)),\n",
    "        ('relu', nn.ReLU()),\n",
    "      ]))\n",
    "\n",
    "      self.conv_block2 = nn.Sequential(OrderedDict([\n",
    "        ('conv', nn.Conv1d(256,256,5, padding=2)),\n",
    "        ('norm', nn.InstanceNorm1d(256)),\n",
    "        ('relu', nn.ReLU()),\n",
    "      ]))\n",
    "\n",
    "      self.lstm = nn.LSTM(256, 512, num_layers=2, bidirectional= True)\n",
    "      self.pool = nn.MaxPool1d(5, 2, 2)\n",
    "      \n",
    "      self.final_layer = nn.Linear(3548 * 512, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "      x = self.conv_block1(x)\n",
    "      x = self.conv_block2(x)\n",
    "\n",
    "      x = torch.swapaxes(x, 1, 2)\n",
    "\n",
    "      x, _ = self.lstm(x)\n",
    "      x = self.pool(x)\n",
    "\n",
    "      x = torch.flatten(x, start_dim=1)\n",
    "\n",
    "      x = self.final_layer(x)\n",
    "\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifierModel(\n",
       "  (conv_block1): Sequential(\n",
       "    (conv): Conv1d(768, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (norm): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (conv_block2): Sequential(\n",
       "    (conv): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (norm): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (lstm): LSTM(256, 512, num_layers=2, bidirectional=True)\n",
       "  (pool): MaxPool1d(kernel_size=5, stride=2, padding=2, dilation=1, ceil_mode=False)\n",
       "  (final_layer): Linear(in_features=1816576, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in the Model\n",
    "model = ClassifierModel()\n",
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick Sanity Check: We can access a datapoint and run it through the model, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1135467])\n",
      "torch.Size([1, 768, 3548])\n",
      "tensor([[-0.0053,  0.0066,  0.0014,  0.0083, -0.0193]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "wavform, label = train_dataset.__getitem__(1)\n",
    "wavform = wavform.to(device).unsqueeze(0)\n",
    "print(wavform.shape)\n",
    "\n",
    "# Get the features\n",
    "sample = process_wavform(wavform, hubert_model)\n",
    "\n",
    "sample = sample.unsqueeze(0) # Add a dimension to simulate batch_size\n",
    "out = model(sample)\n",
    "print(sample.shape)\n",
    "print(out)\n",
    "\n",
    "del sample \n",
    "del out\n",
    "del wavform"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many parameters does our neural network have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Params: 19847685\n"
     ]
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Number of Params: %s\"% pytorch_total_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have our dataset, we have our model--what's next? The training loop!\n",
    "\n",
    "Modern deep learning methods use learning algorithms, usually based off of [Stochastic Gradient Descent (SGD)](https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31), that guide the parameters of a neural network towards configurations that yield meaningful connections between the input and output. For this task, we're going to use a simple L2 (MSE) loss coupled with the Adam Optimizer (a variant of SGD with Momentum).  \n",
    "\n",
    "Ask yourself though, is this the right loss? Are there better losses out there? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Optimizer\n",
    "# First, put in the parameters you want to update (in this case, all of the parameters)\n",
    "# Learning Rate: Also known as the step size. It's basically how big of updates we want to take. Finding the right learning rate is an art\n",
    "# Weight Decay: Essentially an L2 Regularization on the weights. Helps keep the parameters small and under control\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001, weight_decay=1e-4) \n",
    "\n",
    "# Create the criterion or loss function, we'll be using a simple MSE loss\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Epochs: Aka the number of times you want to load through the dataset\n",
    "# Going to have to experiment a bit to see what your computer can handle!\n",
    "epochs = 10 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training Loop\n",
    "\n",
    "The nice thing about PyTorch is that, no matter how complicated the model or parallel computing setup or training procedure, the training loop is going to look roughly the same. The pseudocode is usually going to look like this:\n",
    "\n",
    "```\n",
    "for epoch in epoch_num:\n",
    "    for batch in dataloader:\n",
    "        // Zero the Gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        // Extract the inputs and labels from the batch\n",
    "        inputs, labels = batch\n",
    "\n",
    "        // Preprocess the Data\n",
    "        inputs = process_data(inputs)\n",
    "\n",
    "        // Forward Pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        // Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        // Backward pass and update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "```\n",
    "\n",
    "And that's it! There might be some slight variations, but, for the most part, every PyTorch training loop is going to have this same structure. At least for me, I usually start looking at the training loop when I'm trying to understand a deep learning codebase since it allows you to track the flow of the data through the model and processing. A simple but important fact: Everything that you need to pay attention to when looking how to train a model will be present in the training loop\n",
    "\n",
    "\n",
    "Using the above pseudocode, let's try and implement our own training loop! I provided the for loops for you. tqdm is a nice package for visualizing a progress bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1135467] at entry 0 and [2, 1135467] at entry 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39m# Create the progress bar\u001b[39;00m\n\u001b[1;32m     11\u001b[0m progress \u001b[39m=\u001b[39m tqdm(data_loader)\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfor\u001b[39;00m i, (waves, labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(progress):\n\u001b[1;32m     13\u001b[0m     \u001b[39m# TODO: Implement the training loop based on the pseudocode provided above\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     17\u001b[0m     inputs \u001b[39m=\u001b[39m process_wavform(waves, hubert_model)\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[1;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1135467] at entry 0 and [2, 1135467] at entry 4"
     ]
    }
   ],
   "source": [
    "## The Training Loop!\n",
    "model.train() # Put our model into training mode (important! This can effect how the model is run)\n",
    "avg_losses = [] # Keep track of the loss per epoch!\n",
    "step = 0 # A variable to let us know how many updates we've done\n",
    "for epoch in range(0, epochs):\n",
    "    # Track the average loss per epoch\n",
    "    avg_loss = 0\n",
    "    avg_loss_step = 0\n",
    "\n",
    "    # Create the progress bar\n",
    "    progress = tqdm(data_loader)\n",
    "    for i, (waves, labels) in enumerate(progress):\n",
    "        # TODO: Implement the training loop based on the pseudocode provided above\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs = process_wavform(waves, hubert_model).to(device)\n",
    "\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs).float()\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Bookkeeping and Setting the Progress Bar\n",
    "        avg_loss += loss.item()\n",
    "        avg_loss_step += 1\n",
    "        step += 1\n",
    "\n",
    "        progress.set_postfix(\n",
    "            loss = avg_loss / avg_loss_step,\n",
    "            epoch = epoch + i / len(data_loader)\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
