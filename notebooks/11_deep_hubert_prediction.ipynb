{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PQ Prediction with HuBERT using a Deep Neural Net \n",
    "\n",
    "In the last notebook, we used HuBERT layers to predict the PQ Representation of the audio clips in the Perceptual Voice Qualities Database (PVQD). We used a naive aggregation technique to do this (simply averaging over the time steps), but what if instead we decided to try and take advantage of the temporal information to perform prediction? \n",
    "\n",
    "We may *want* to use deep neural networks, because they are super duper cool and powerful, but we have to ask if it is the best tool for the job. Given that the PQ Representation is a vector that represents speaker identity, which can be thought of as the time-invariant characteristics of speech (mostly), will the temporal information increase predictive performance? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.utils import *\n",
    "\n",
    "import time\n",
    "from collections import OrderedDict # Not necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Rate: 16000\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Load in the HuBERT Model\n",
    "torch.random.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "bundle = torchaudio.pipelines.HUBERT_BASE\n",
    "hubert_model = bundle.get_model().to(device)\n",
    "\n",
    "print(\"Sample Rate:\", bundle.sample_rate)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the Data \n",
    "## Load in the DataFrame\n",
    "y_train = pd.read_csv('../data/pvqd/train_test_split/y_train.csv', index_col=0)\n",
    "y_val = pd.read_csv('../data/pvqd/train_test_split/y_val.csv', index_col=0)\n",
    "\n",
    "# Same inefficient code as before!\n",
    "\n",
    "data_path = \"../data/pvqd/audio_clips/\"\n",
    "audio_files = os.listdir(data_path)\n",
    "speaker_ids = [extract_speaker(audio_file) for audio_file in audio_files]\n",
    "\n",
    "# Assertion to make sure speaker_ids matches y_train['File']\n",
    "i = 0\n",
    "for spk_id in y_train[\"File\"]:\n",
    "    try:\n",
    "        assert spk_id in speaker_ids\n",
    "    except:\n",
    "        print(spk_id)\n",
    "        i+=1\n",
    "\n",
    "# Dictionary to Link Speaker ID to Audio File for O(1) access\n",
    "speaker_file_dict = {}\n",
    "for i in range(0, len(speaker_ids)):\n",
    "    speaker_file_dict[speaker_ids[i]] = os.path.join(data_path, audio_files[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First things first, we gotta make the data compatible with PyTorch \n",
    "\n",
    "PyTorch is a very powerful package that allows us to process data in fast and remarkable ways. Now, the way to take advantage of PyTorch's [Dataset](https://pytorch.org/tutorials/ beginner/data_loading_tutorial.html) class. Using this class, you can train in parallel, and easily modify batch size (the number of input samples your network considers per training update)\n",
    "\n",
    "Below, we're going to load in the hubert model and implement the class HubertDataset, which will take in a a dataframe of labels, the speaker-to-wavfile array, and a hubert model, and will return the sixth layer of the hubert features and the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Implement the Dataset function __getitem__ \n",
    "\n",
    "class HubertDataset(Dataset):\n",
    "    def __init__(self, dataframe, spk_wav_arr, hubert_model, device):\n",
    "        self.dataframe = dataframe\n",
    "        self.spk_wav_arr = spk_wav_arr # array that links speaker id and \n",
    "        self.hubert_model = hubert_model # the hubert model\n",
    "        self.hubert_model.eval() # Ensure it's in eval mode\n",
    "        self.device = device # should we be doing this?\n",
    "        self.pad_len = 3548 # Max seq length\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of data samples\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    # TODO: Implement the __getitem__ function\n",
    "    # Given an index, return the features and the labels\n",
    "    def __getitem__(self, index):\n",
    "        # Retrieve a single row from the DataFrame\n",
    "        single_row = self.dataframe.values[index, :]\n",
    "        speaker_id = single_row[0]\n",
    "        audio_file = self.spk_wav_arr[speaker_id]\n",
    "\n",
    "        waveform, sample_rate = torchaudio.load(audio_file)\n",
    "        waveform = waveform.to(self.device) # Question: Should we be processing the waveform here?\n",
    "\n",
    "        if sample_rate != bundle.sample_rate:\n",
    "            waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)\n",
    "\n",
    "        # Extract the HuBERT layers like before\n",
    "        features = self.hubert_model.extract_features(waveform)\n",
    "        features = features[0]\n",
    "\n",
    "        features = features[5]\n",
    "\n",
    "        # Have to swap the axes, since we want the hubert dimension to be the channels\n",
    "        features = torch.swapaxes(features, 1, 2)\n",
    "\n",
    "        to_pad = self.pad_len - features.shape[-1]\n",
    "        features = F.pad(features, (0, to_pad))\n",
    "\n",
    "        # Let's take a fixed length\n",
    "\n",
    "        # Load in the Labels\n",
    "        labels = torch.from_numpy(single_row[1:].astype(float)).to(self.device)\n",
    "\n",
    "        # Process the row (assuming you have a function that does this)\n",
    "\n",
    "        return features, labels\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: PyTorch's Dataset class is *incredibly* flexible\n",
    "\n",
    "There's no need to make it load a pandas DataFrame at all. You could store everything on disk, and have the index be associated with a file location.\n",
    "\n",
    "Let's use our created HubertDataset class to create a DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HubertDataset(y_train, speaker_file_dict, hubert_model, device)\n",
    "\n",
    "batch_size = 8 # Set the batch size\n",
    "data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Moment You've All Been Waiting for: Let's Build a Neural Network\n",
    "\n",
    "For the purposes of this workshop, we're going to just build a convolutional neural network. There are other ways to model speech or sequential data more broadly, such as using a Transformer (or a Conformer if you want to combine the two). We're going to keep it simple. Feel free to modify the network though, Torchaudio makes it pretty easy! (https://pytorch.org/audio/main/generated/torchaudio.models.Conformer.html for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "\n",
    "      self.conv_block1 = nn.Sequential(OrderedDict([\n",
    "        ('conv', nn.Conv1d(768,256,5, padding=2)),\n",
    "        ('norm', nn.InstanceNorm1d(256)),\n",
    "        ('relu', nn.ReLU()),\n",
    "      ]))\n",
    "\n",
    "      self.conv_block2 = nn.Sequential(OrderedDict([\n",
    "        ('conv', nn.Conv1d(256,256,5, padding=2)),\n",
    "        ('norm', nn.InstanceNorm1d(256)),\n",
    "        ('relu', nn.ReLU()),\n",
    "      ]))\n",
    "\n",
    "      self.lstm = nn.LSTM(256, 512, num_layers=2, bidirectional= True)\n",
    "      self.pool = nn.MaxPool1d(5, 2, 2)\n",
    "      \n",
    "      self.final_layer = nn.Linear(3548 * 512, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "      x = self.conv_block1(x)\n",
    "      x = self.conv_block2(x)\n",
    "\n",
    "      x = torch.swapaxes(x, 1, 2)\n",
    "\n",
    "      x, _ = self.lstm(x)\n",
    "      x = self.pool(x)\n",
    "\n",
    "      x = torch.flatten(x, start_dim=1)\n",
    "\n",
    "      x = self.final_layer(x)\n",
    "\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifierModel(\n",
       "  (conv_block1): Sequential(\n",
       "    (conv): Conv1d(768, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (norm): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (conv_block2): Sequential(\n",
       "    (conv): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (norm): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (lstm): LSTM(256, 512, num_layers=2, bidirectional=True)\n",
       "  (pool): MaxPool1d(kernel_size=5, stride=2, padding=2, dilation=1, ceil_mode=False)\n",
       "  (final_layer): Linear(in_features=1816576, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ClassifierModel()\n",
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, label = train_dataset.__getitem__(1)\n",
    "out = model(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768, 3548])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3548"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
