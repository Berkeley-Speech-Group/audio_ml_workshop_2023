{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hubert Prediction\n",
    "\n",
    "A notebook to explore using Hubert. Based partially off of our work on Perceptual Qualities (https://arxiv.org/abs/2310.02497)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Uncomment the below if you have a CUDA device/multiple\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.utils import *\n",
    "\n",
    "# Importing Sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = torchaudio.pipelines.HUBERT_BASE\n",
    "# bundle = torchaudio.pipelines.HUBERT_XLARGE\n",
    "\n",
    "\n",
    "print(\"Sample Rate:\", bundle.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bundle.get_model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's Load in a Specific File\n",
    "SPEECH_FILE = \"../data/pvqd/audio_clips/BL10_ENSS.wav\"\n",
    "IPython.display.Audio(SPEECH_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, sample_rate = torchaudio.load(SPEECH_FILE)\n",
    "waveform = waveform.to(device)\n",
    "\n",
    "if sample_rate != bundle.sample_rate:\n",
    "    waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    features = model.extract_features(waveform)\n",
    "    features = features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(len(features), 1, figsize=(16, 4.3 * len(features)))\n",
    "# for i, feats in enumerate(features):\n",
    "#     ax[i].imshow(feats[0].cpu(), interpolation=\"nearest\")\n",
    "#     ax[i].set_title(f\"Feature from transformer layer {i+1}\")\n",
    "#     ax[i].set_xlabel(\"Feature dimension\")\n",
    "#     ax[i].set_ylabel(\"Frame (time-axis)\")\n",
    "    \n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension is BxTxD \n",
    "sixth_layer = features[5]\n",
    "sixth_layer.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Use HuBERT for Some Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in the DataFrame\n",
    "y_train = pd.read_csv('../data/pvqd/train_test_split/y_train.csv', index_col=0)\n",
    "y_val = pd.read_csv('../data/pvqd/train_test_split/y_val.csv', index_col=0)\n",
    "\n",
    "# Load in the Test Set below if you want it\n",
    "\n",
    "y_train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to predict the five perceptual qualities: Breathiness, Loudness, Pitch, Roughness, and Strain. First, we have to link the files with their labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linking the `File` column with the actual File Name\n",
    "# NOTE: This code is inefficient--why is it inefficient and how can we make it faster?\n",
    "\n",
    "data_path = \"../data/pvqd/audio_clips/\"\n",
    "audio_files = os.listdir(data_path)\n",
    "speaker_ids = [extract_speaker(audio_file) for audio_file in audio_files]\n",
    "\n",
    "# Assertion to make sure speaker_ids matches y_train['File']\n",
    "i = 0\n",
    "for spk_id in y_train[\"File\"]:\n",
    "    try:\n",
    "        assert spk_id in speaker_ids\n",
    "    except:\n",
    "        print(spk_id)\n",
    "        i+=1\n",
    "\n",
    "# Dictionary to Link Speaker ID to Audio File for O(1) access\n",
    "speaker_file_dict = {}\n",
    "for i in range(0, len(speaker_ids)):\n",
    "    speaker_file_dict[speaker_ids[i]] = os.path.join(data_path, audio_files[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What model and features should we use?\n",
    "\n",
    "As we saw earlier, there's a lot of layers in HuBERT, and each one contains different information (https://arxiv.org/pdf/2211.03929.pdf). The choice between layer 3 or layer 8 could result in vastly different predictive performance. Then there's the question of how we should even process the features! Should we use the raw shape of Batch-Size x Time-Dim x Feature Dim, or should we do some aggregation and perform a simple linear regression on top of that?\n",
    "\n",
    "When it comes to machine learning and modeling, there's no reason not to try them all if you have the time (provided you've done the proper train-validation-test split first!). If you don't have the time, you have to use your best judgement to decide which archietctures, objective functions, and feature representations to explore first. This intuition comes with time.\n",
    "\n",
    "To start off with, let's use Scikit-Learn (https://scikit-learn.org/stable/) to run some simple regression models (Linear, Lasso, and Random Forest) on a naive averaged version of the sixth HuBERT layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the features and turn it into a dataframe\n",
    "# NOTE: Is this the most efficient way to do this? How can we refactor this code? Should we be storing all this data in memory?\n",
    "\n",
    "X_train = []\n",
    "X_val = []\n",
    "\n",
    "# Construct X_train\n",
    "start_time = time.time() # Let's do some simple profiling\n",
    "for spk_id in y_train[\"File\"]:\n",
    "    audio_file = speaker_file_dict[spk_id]\n",
    "\n",
    "    # Load in the Audio and \n",
    "    waveform, sample_rate = torchaudio.load(audio_file)\n",
    "    waveform = waveform.to(device)\n",
    "\n",
    "    if sample_rate != bundle.sample_rate:\n",
    "        waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)\n",
    "\n",
    "    # Extract the HuBERT layers like before\n",
    "    with torch.inference_mode():\n",
    "        features = model.extract_features(waveform)\n",
    "        features = features[0]\n",
    "\n",
    "    sixth_layer = features[5]\n",
    "\n",
    "    # TODO: Take the the mean over the Time dimension\n",
    "    # Link: https://pytorch.org/docs/stable/generated/torch.mean.html \n",
    "\n",
    "\n",
    "\n",
    "    # Scikit learn doesn't work with Tensors, so we load them onto the CPU \n",
    "    # and convert to Numpy\n",
    "    features = sixth_layer_mean[0].cpu().numpy() \n",
    "    X_train.append(features)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Finished building X_train in %s seconds\" % round(elapsed_time, 2))\n",
    "\n",
    "# Construct X_val\n",
    "start_time = time.time() # Let's do some simple profiling\n",
    "for spk_id in y_val[\"File\"]:\n",
    "    audio_file = speaker_file_dict[spk_id]\n",
    "\n",
    "    # Load in the Audio and \n",
    "    waveform, sample_rate = torchaudio.load(audio_file)\n",
    "    waveform = waveform.to(device)\n",
    "\n",
    "    if sample_rate != bundle.sample_rate:\n",
    "        waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)\n",
    "\n",
    "    # Extract the HuBERT layers like before\n",
    "    with torch.inference_mode():\n",
    "        features = model.extract_features(waveform)\n",
    "        features = features[0]\n",
    "\n",
    "    sixth_layer = features[5]\n",
    "\n",
    "    # TODO: Take the the mean over the Time dimension\n",
    "    # Link: https://pytorch.org/docs/stable/generated/torch.mean.html \n",
    "\n",
    "\n",
    "    # Scikit learn doesn't work with Tensors, so we load them onto the CPU \n",
    "    # and convert to Numpy\n",
    "    features = sixth_layer_mean[0].cpu().numpy() \n",
    "    X_val.append(features)\n",
    "\n",
    "X_val = np.array(X_val)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Finished building X_val in %s seconds\" % round(elapsed_time, 2))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train some models!\n",
    "\n",
    "Sanity Check: Are you absolutely certain that the first row of X_train is associated with the first row of y_train?\n",
    "\n",
    "Below, we're going to train a linear and lasso regression model. How do you think it will do? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Follow the documentation from sklearn.linear_model.Linear_Regression (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) \n",
    "## to train a Linear and Lasso Regression\n",
    "np.random.seed(42) # Set the seed so that we all get the same results--hopefully\n",
    "\n",
    "# TODO: Create and train a Linear Regression Model called reg\n",
    "reg = ???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use Reg to Predict X_train and evaluate\n",
    "\n",
    "preds = ???\n",
    "assert preds.shape == y_val.drop([\"File\"], axis=1).shape # Sanity check assertion\n",
    "\n",
    "# TODO: Calculate the Mean Squared Error with mean_squared_error--Wait, is this right?\n",
    "rmse = ???\n",
    "print(\"Linear Model RMSE %s\" % rmse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We took the error over averaged oer every PQ! Let's try computing it for each PQ individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Calculate the RMSE Column-wise\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's now train a Lasso\n",
    "\n",
    "Some question: What's a lasso regression? Why would we want to try a lasso regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create and train a Lasso Regression Model called lasso_reg with alpha = 0.1\n",
    "lasso_reg = ???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use lasso_reg to Predict X_train and evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Calculate the RMSE Column-wise\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "How does linear regression vs. lasso regression perform, based on RMSE?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Try Random Forest Regression\n",
    "\n",
    "Linear models are nice and all, but they're just that--linear. Speech is an incredibly complex phenomena that can't possibly represented by linear relationships (right?). Let's see how linear models compare against [random forests](https://en.wikipedia.org/wiki/Random_forest), a common ensemble method that is particularly handy at characterizing non-linear relationships.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train a random forest model using [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n",
    "# Have you noticed a pattern with Sklearn yet?\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "rf_reg = ??? # TODO\n",
    "end_time = time.time()\n",
    "print(\"Finished Training RF in %s Seconds\" % (end_time- start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use rf_reg to Predict X_train and evaluate\n",
    "\n",
    "rf_preds = ???\n",
    "assert rf_preds.shape == y_val.drop([\"File\"], axis=1).shape # Sanity check assertion\n",
    "\n",
    "# Calculate the Mean Squared Error--Wait, is this right?\n",
    "rmse = \n",
    "print(\"Random Forest Model RMSE %s\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Calculate the RMSE Column-wise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways and Discussion\n",
    "\n",
    "So, which model performed the best? Can you tell? How might you further investigate to further guarantee that one model performs better than another? (Hint: Look into bootstrapping)\n",
    "\n",
    "What happens if you vary all the different models we chose? Different hyperparameters, different aggregation (why just do the mean?), different layers, a different representation altogether?\n",
    "\n",
    "In a lot of Machine Learning problems, not just in Speech, there are decisions after decisions to be made and varied. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Such Exploration: What happens if we vary the layer we examine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the features and turn it into a dataframe\n",
    "# NOTE: Is this the most efficient way to do this? How can we refactor this code? Should we be storing all this data in memory?\n",
    "\n",
    "X_trains = []\n",
    "X_vals = []\n",
    "\n",
    "# Construct X_train\n",
    "start_time = time.time() # Let's do some simple profiling\n",
    "for spk_id in y_train[\"File\"]:\n",
    "    audio_file = speaker_file_dict[spk_id]\n",
    "\n",
    "    # Load in the Audio and \n",
    "    waveform, sample_rate = torchaudio.load(audio_file)\n",
    "    waveform = waveform.to(device)\n",
    "\n",
    "    if sample_rate != bundle.sample_rate:\n",
    "        waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)\n",
    "\n",
    "    # Extract the HuBERT layers like before\n",
    "    with torch.inference_mode():\n",
    "        features = model.extract_features(waveform)\n",
    "        features = features[0]\n",
    "\n",
    "    all_layer_means = [layer.mean(dim=1)[0].cpu().numpy() for layer in features]\n",
    "\n",
    "    X_trains.append(all_layer_means)\n",
    "\n",
    "    \n",
    "\n",
    "X_trains = np.array(X_trains)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Finished building X_train in %s seconds\" % round(elapsed_time, 2))\n",
    "\n",
    "# Construct X_val\n",
    "start_time = time.time() # Let's do some simple profiling\n",
    "for spk_id in y_val[\"File\"]:\n",
    "    audio_file = speaker_file_dict[spk_id]\n",
    "\n",
    "    # Load in the Audio and \n",
    "    waveform, sample_rate = torchaudio.load(audio_file)\n",
    "    waveform = waveform.to(device)\n",
    "\n",
    "    if sample_rate != bundle.sample_rate:\n",
    "        waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        features = model.extract_features(waveform)\n",
    "        features = features[0]\n",
    "\n",
    "    all_layer_means = [layer.mean(dim=1)[0].cpu().numpy() for layer in features]\n",
    "\n",
    "    X_vals.append(all_layer_means)\n",
    "\n",
    "X_vals = np.array(X_vals)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Finished building X_val in %s seconds\" % round(elapsed_time, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trains.shape, X_vals.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each layer, we're going to train a linear regression and see how the error changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_err_dict = {\n",
    "    \"layer\": [],\n",
    "    \"avg_rmse\": []\n",
    "}\n",
    "\n",
    "for layer_num in range(0, X_trains.shape[1]):\n",
    "    X_train = X_trains[:, layer_num, :]\n",
    "    X_val = X_vals[:, layer_num, :]\n",
    "\n",
    "    reg = LinearRegression().fit(X_train, y_train.drop([\"File\"], axis=1))\n",
    "\n",
    "    preds = reg.predict(X_val)\n",
    "    assert preds.shape == y_val.drop([\"File\"], axis=1).shape # Sanity check assertion\n",
    "\n",
    "    # Calculate the Mean Squared Error--Wait, is this right?\n",
    "    rmse = mean_squared_error(y_val.drop([\"File\"], axis=1).values, preds, squared=False)\n",
    "    \n",
    "    layer_err_dict[\"layer\"].append(layer_num)\n",
    "    layer_err_dict[\"avg_rmse\"].append(rmse)\n",
    "\n",
    "layer_err_df = pd.DataFrame(layer_err_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=layer_err_df, x=\"layer\", y=\"avg_rmse\")\n",
    "plt.ylim([0, 20])\n",
    "plt.title(\"Avg. RMSE over HuBERT Layer\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congrats! We just did research"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aligner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
