{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PQ Prediction with HuBERT using a Deep Neural Net \n",
    "\n",
    "In the last notebook, we used HuBERT layers to predict the PQ Representation of the audio clips in the Perceptual Voice Qualities Database (PVQD). We used a naive aggregation technique to do this (simply averaging over the time steps), but what if instead we decided to try and take advantage of the temporal information to perform prediction? \n",
    "\n",
    "We may *want* to use deep neural networks, because they are super duper cool and powerful, but we have to ask if it is the best tool for the job. Given that the PQ Representation is a vector that represents speaker identity, which can be thought of as the time-invariant characteristics of speech (mostly), will the temporal information increase predictive performance? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# Uncomment the line below if you have a CUDA device or multiple\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.utils import *\n",
    "\n",
    "import time\n",
    "from collections import OrderedDict # Not necessary\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the HuBERT Model\n",
    "torch.random.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "bundle = torchaudio.pipelines.HUBERT_BASE\n",
    "hubert_model = bundle.get_model().to(device)\n",
    "hubert_model.eval() # make sure it's in eval mode\n",
    "\n",
    "print(\"Sample Rate:\", bundle.sample_rate)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the Data \n",
    "## Load in the DataFrame\n",
    "y_train = pd.read_csv('../data/pvqd/train_test_split/y_train.csv', index_col=0)\n",
    "y_val = pd.read_csv('../data/pvqd/train_test_split/y_val.csv', index_col=0)\n",
    "\n",
    "# Same inefficient code as before!\n",
    "\n",
    "data_path = \"../data/pvqd/audio_clips/\"\n",
    "audio_files = os.listdir(data_path)\n",
    "speaker_ids = [extract_speaker(audio_file) for audio_file in audio_files]\n",
    "\n",
    "# Assertion to make sure speaker_ids matches y_train['File']\n",
    "i = 0\n",
    "for spk_id in y_train[\"File\"]:\n",
    "    try:\n",
    "        assert spk_id in speaker_ids\n",
    "    except:\n",
    "        print(spk_id)\n",
    "        i+=1\n",
    "\n",
    "# Dictionary to Link Speaker ID to Audio File for O(1) access\n",
    "speaker_file_dict = {}\n",
    "for i in range(0, len(speaker_ids)):\n",
    "    speaker_file_dict[speaker_ids[i]] = os.path.join(data_path, audio_files[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First things first, we gotta make the data compatible with PyTorch \n",
    "\n",
    "PyTorch is a very powerful package that allows us to process data in fast and remarkable ways. Now, the way to take advantage of PyTorch's [Dataset](https://pytorch.org/tutorials/ beginner/data_loading_tutorial.html) class. Using this class, you can train in parallel, and easily modify batch size (the number of input samples your network considers per training update)\n",
    "\n",
    "Below, we're going to load in the hubert model and implement the class HubertDataset, which will take in a a dataframe of labels, the speaker-to-wavfile array, and a hubert model, and will return the sixth layer of the hubert features and the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Implement the Dataset function __getitem__ \n",
    "\n",
    "class HubertDataset(Dataset):\n",
    "    def __init__(self, dataframe, spk_wav_arr, hubert_model, device):\n",
    "        self.dataframe = dataframe\n",
    "        self.spk_wav_arr = spk_wav_arr # array that links speaker id and \n",
    "        self.pad_len = 1135467 # Max seq length\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of data samples\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    # TODO: Implement the __getitem__ function\n",
    "    # Given an index, return the features and the labels\n",
    "    def __getitem__(self, index):\n",
    "        # Retrieve a single row from the DataFrame\n",
    "        # TODO: Load in the waveform\n",
    "\n",
    "\n",
    "        # Let's take a fixed length-- pad the waveform with 0s up to the max file length\n",
    "        # Question: Should we be padding with 0s?\n",
    "        to_pad = self.pad_len - waveform.shape[-1]\n",
    "        waveform = F.pad(waveform, (0, to_pad))\n",
    "\n",
    "        # TODO: Downmix Stereo to Mono & Get rid of the channels dimension\n",
    "        if len(waveform.shape) == 2 and (waveform.shape[0] == 2):\n",
    "            pass\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # Load in the Labels\n",
    "        labels = torch.from_numpy(single_row[1:].astype(float))\n",
    "\n",
    "        \n",
    "\n",
    "        return waveform, labels\n",
    "\n",
    "# Util function to help out with HuBERT processing\n",
    "def process_wavform(waveform, hubert_model):\n",
    "    # Extract the HuBERT layers like before\n",
    "    features = hubert_model.extract_features(waveform)\n",
    "    features = features[0]\n",
    "\n",
    "    features = features[5]\n",
    "\n",
    "    # Have to swap the axes, since we want the hubert dimension to be the channels\n",
    "    features = torch.swapaxes(features, 1, 2)\n",
    "\n",
    "    # Remove the first dimension\n",
    "    features = features.squeeze(0)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: PyTorch's Dataset class is *incredibly* flexible\n",
    "\n",
    "There's no need to make it load a pandas DataFrame at all. You could store everything on disk, and have the index be associated with a file location.\n",
    "\n",
    "Let's use our created HubertDataset class to create a DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HubertDataset(y_train, speaker_file_dict, hubert_model, device)\n",
    "\n",
    "batch_size = 8 # Set the batch size\n",
    "data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Moment You've All Been Waiting for: Let's Build a Neural Network\n",
    "\n",
    "For the purposes of this workshop, we're going to just build a convolutional neural network. There are other ways to model speech or sequential data more broadly, such as using a Transformer (or a Conformer if you want to combine the two). We're going to keep it simple. Feel free to modify the network though, Torchaudio makes it pretty easy! (https://pytorch.org/audio/main/generated/torchaudio.models.Conformer.html for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This class is not implemented, how do I put this, well. How would you change this class to make it more flexible and better?\n",
    "class ClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "\n",
    "      self.conv_block1 = nn.Sequential(OrderedDict([\n",
    "        ('conv', nn.Conv1d(768,256,5, padding=2)),\n",
    "        ('norm', nn.InstanceNorm1d(256)),\n",
    "        ('relu', nn.ReLU()),\n",
    "      ]))\n",
    "\n",
    "      self.conv_block2 = nn.Sequential(OrderedDict([\n",
    "        ('conv', nn.Conv1d(256,256,5, padding=2)),\n",
    "        ('norm', nn.InstanceNorm1d(256)),\n",
    "        ('relu', nn.ReLU()),\n",
    "      ]))\n",
    "\n",
    "      self.lstm = nn.LSTM(256, 512, num_layers=2, bidirectional= True)\n",
    "      self.pool = nn.MaxPool1d(5, 2, 2)\n",
    "      \n",
    "      self.final_layer = nn.Linear(3548 * 512, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "      x = self.conv_block1(x)\n",
    "      x = self.conv_block2(x)\n",
    "\n",
    "      x = torch.swapaxes(x, 1, 2)\n",
    "\n",
    "      x, _ = self.lstm(x)\n",
    "      x = self.pool(x)\n",
    "\n",
    "      x = torch.flatten(x, start_dim=1)\n",
    "\n",
    "      x = self.final_layer(x)\n",
    "\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the Model\n",
    "model = ClassifierModel()\n",
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick Sanity Check: We can access a datapoint and run it through the model, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavform, label = train_dataset.__getitem__(1)\n",
    "wavform = wavform.to(device).unsqueeze(0)\n",
    "print(wavform.shape)\n",
    "\n",
    "# Get the features\n",
    "sample = process_wavform(wavform, hubert_model)\n",
    "\n",
    "sample = sample.unsqueeze(0) # Add a dimension to simulate batch_size\n",
    "out = model(sample)\n",
    "print(sample.shape)\n",
    "print(out)\n",
    "\n",
    "del sample \n",
    "del out\n",
    "del wavform"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many parameters does our neural network have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Number of Params: %s\"% pytorch_total_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have our dataset, we have our model--what's next? The training loop!\n",
    "\n",
    "Modern deep learning methods use learning algorithms, usually based off of [Stochastic Gradient Descent (SGD)](https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31), that guide the parameters of a neural network towards configurations that yield meaningful connections between the input and output. For this task, we're going to use a simple L2 (MSE) loss coupled with the Adam Optimizer (a variant of SGD with Momentum).  \n",
    "\n",
    "Ask yourself though, is this the right loss? Are there better losses out there? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Optimizer\n",
    "# First, put in the parameters you want to update (in this case, all of the parameters)\n",
    "# Learning Rate: Also known as the step size. It's basically how big of updates we want to take. Finding the right learning rate is an art\n",
    "# Weight Decay: Essentially an L2 Regularization on the weights. Helps keep the parameters small and under control\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001, weight_decay=1e-4) \n",
    "\n",
    "# Create the criterion or loss function, we'll be using a simple MSE loss\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Epochs: Aka the number of times you want to load through the dataset\n",
    "# Going to have to experiment a bit to see what your computer can handle!\n",
    "epochs = 10 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training Loop\n",
    "\n",
    "The nice thing about PyTorch is that, no matter how complicated the model or parallel computing setup or training procedure, the training loop is going to look roughly the same. The pseudocode is usually going to look like this:\n",
    "\n",
    "```\n",
    "for epoch in epoch_num:\n",
    "    for batch in dataloader:\n",
    "        // Zero the Gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        // Extract the inputs and labels from the batch\n",
    "        inputs, labels = batch\n",
    "\n",
    "        // Preprocess the Data\n",
    "        inputs = process_data(inputs)\n",
    "\n",
    "        // Forward Pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        // Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        // Backward pass and update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "```\n",
    "\n",
    "And that's it! There might be some slight variations, but, for the most part, every PyTorch training loop is going to have this same structure. At least for me, I usually start looking at the training loop when I'm trying to understand a deep learning codebase since it allows you to track the flow of the data through the model and processing. A simple but important fact: Everything that you need to pay attention to when looking how to train a model will be present in the training loop\n",
    "\n",
    "\n",
    "Using the above pseudocode, let's try and implement our own training loop! I provided the for loops for you. tqdm is a nice package for visualizing a progress bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The Training Loop!\n",
    "model.train() # Put our model into training mode (important! This can effect how the model is run)\n",
    "hubert_model.eval() # Make sure it's in eval!\n",
    "avg_losses = [] # Keep track of the loss per epoch!\n",
    "losses = []\n",
    "step = 0 # A variable to let us know how many updates we've done\n",
    "for epoch in range(0, epochs):\n",
    "    # Track the average loss per epoch\n",
    "    avg_loss = 0\n",
    "    avg_loss_step = 0\n",
    "\n",
    "    # Create the progress bar\n",
    "    progress = tqdm(data_loader)\n",
    "    for i, (waves, labels) in enumerate(progress):\n",
    "        # TODO: Implement the training loop based on the pseudocode provided above\n",
    "\n",
    "\n",
    "        # Bookkeeping and Setting the Progress Bar\n",
    "        avg_loss += loss.item()\n",
    "        avg_loss_step += 1\n",
    "        step += 1\n",
    "\n",
    "        progress.set_postfix(\n",
    "            loss = avg_loss / avg_loss_step,\n",
    "            epoch = epoch + i / len(data_loader)\n",
    "        )\n",
    "\n",
    "        avg_losses.append(avg_loss / avg_loss_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Plot of the Losses!\n",
    "sns.lineplot(avg_losses)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that we've trained our model, let's evaluate it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = HubertDataset(y_val, speaker_file_dict, hubert_model, device)\n",
    "\n",
    "# Can just use a batch_size of 1, just computing loss\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=1, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put model into eval mode\n",
    "model.eval()\n",
    "\n",
    "losses = torch.zeros((len(val_data_loader), 5)).to(device)\n",
    "with torch.no_grad():\n",
    "    for i, (waves, labels) in enumerate(val_data_loader):\n",
    "\n",
    "\n",
    "        inputs = process_wavform(waves.to(device), hubert_model).to(device).float()\n",
    "        \n",
    "        # If doing a batch_size = 1, might have to unsqueeze\n",
    "        inputs = inputs.unsqueeze(0)\n",
    "\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs).float()\n",
    "\n",
    "        # Append the squares\n",
    "        losses[i,:] = (labels - outputs)**2\n",
    "\n",
    "print(\"Average RMSE Loss %s\" % torch.sqrt(losses.mean()))\n",
    "print(\"RMSE per PQ %s\" % torch.sqrt(losses.mean(axis = 0)))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways and Questions\n",
    "\n",
    "Congratulations! You trained and designed a neural network for prediction. This is essentially a big part of what it means to do \"deep learning\" research. There's a lot of questions left over, improvements to be done--generally, there's a ton to try and work on. \n",
    "\n",
    "Here are some questions to leave you with:\n",
    "* What happens if you try different architectures? Different learning rates? These things all take time to explore, and, as you work with these tools more, you'll develop an intution.\n",
    "* How might you speed up training... besides just using a GPU! There's [parallelization](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html), [autocasting](https://pytorch.org/docs/stable/amp.html), [quantization](https://pytorch.org/blog/quantization-in-practice/), and much more! But more simply, how might restructure the data, or the above code, to run faster more generally? Ex--what if we took in less data, or explored truncation instead of padding?\n",
    "* In the end, was it worth using a deep learning model for this prediction task? Is it better to stick with the classic ML methods, or do you think more performance can be squeezed out of neural nets?\n",
    "\n",
    "### One final note\n",
    "\n",
    "It may not seem like it, but what you just did was research! I designed this notebook to explore something I had been wanting to do for a bit, namely how we could use HuBERT's representations for prediction of perceptual qualities. I honestly didn't know this would turn out, and, at time of writing, arguably still don't. By completing this notebook, you've explored something that people haven't really explored before! And that's all research realy is. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
